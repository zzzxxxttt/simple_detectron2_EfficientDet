from copy import deepcopy

import math
import fvcore.nn.weight_init as weight_init
import torch.nn.functional as F

import torch
from torch import nn

from detectron2.layers import Conv2d, ShapeSpec, get_norm

from detectron2.modeling.backbone import Backbone
from detectron2.modeling.backbone import BACKBONE_REGISTRY
from detectron2.modeling.backbone.resnet import build_resnet_backbone

from .backbone import build_efficientnet_backbone

__all__ = ["build_retinanet_resnet_bifpn_backbone", "BiFPN"]


class ResampleFeature(nn.Module):
  def __init__(self, in_channels, out_channels, in_stride, out_stride, norm):
    super().__init__()
    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=norm == '')
    self.norm = get_norm(norm, out_channels) if norm != '' else nn.Sequential()

    if out_stride > in_stride:
      s = out_stride // in_stride
      self.resample = nn.MaxPool2d(kernel_size=s + 1, stride=s, padding=s // 2)
    elif out_stride < in_stride:
      s = in_stride // out_stride
      self.resample = nn.Upsample(scale_factor=s)
    else:
      self.resample = nn.Sequential()

    weight_init.c2_xavier_fill(self.conv)

  def forward(self, x):
    return self.resample(self.norm(self.conv(x)))


class ResampleFeature_v2(nn.Module):
  def __init__(self, in_channels, out_channels, norm):
    super().__init__()
    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=norm == '')
    self.norm = get_norm(norm, out_channels) if norm != '' else nn.Sequential()

    weight_init.c2_xavier_fill(self.conv)

  def forward(self, x, target_size):
    out = self.norm(self.conv(x))
    if out.shape[-1] != target_size[-1] or out.shape[-2] != target_size[-2]:
      out = F.interpolate(out, target_size)
    return out


class Attention(nn.Module):
  def __init__(self, num_inputs):
    super().__init__()
    self.num_inputs = num_inputs
    self.atten_w = nn.Parameter(torch.ones(num_inputs))

  def forward(self, x):
    assert isinstance(x, list) and len(x) == self.num_inputs
    atten_w = F.relu(self.atten_w)
    return sum(x_ * w for x_, w in zip(x, atten_w)) / (atten_w.sum() + 1e-3)


class OutputConv(nn.Module):
  def __init__(self, in_channels, out_channels, norm):
    super().__init__()
    self.depthwise = nn.Conv2d(in_channels, out_channels,
                               kernel_size=3, stride=1, padding=1,
                               groups=out_channels, bias=False)
    self.pointwise = nn.Conv2d(out_channels, out_channels,
                               kernel_size=1, stride=1, padding=0, bias=norm == '')
    self.norm = get_norm(norm, out_channels) if norm != '' else nn.Sequential()

    weight_init.c2_xavier_fill(self.depthwise)
    weight_init.c2_xavier_fill(self.pointwise)

  def forward(self, x):
    return self.norm(self.pointwise(self.depthwise(F.relu(x))))


class BiFPN(Backbone):
  """
  This module implements Bi-Derectional Feature Pyramid Network.
  It creates pyramid features built on top of some input feature maps.
  """

  def __init__(self,
               bottom_up,
               in_features,
               out_channels,
               fpn_repeat,
               norm="SyncBN",
               top_block=None,
               fuse_type="sum"):
    """
    Args:
        bottom_up (Backbone): module representing the bottom up subnetwork.
            Must be a subclass of :class:`Backbone`. The multi-scale feature
            maps generated by the bottom up network, and listed in `in_features`,
            are used to generate FPN levels.
        in_features (list[str]): names of the input feature maps coming
            from the backbone to which FPN is attached. For example, if the
            backbone produces ["res2", "res3", "res4"], any *contiguous* sublist
            of these may be used; order must be from high to low resolution.
        out_channels (int): number of channels in the output feature maps.
        norm (str): the normalization to use.
        top_block (nn.Module or None): if provided, an extra operation will
            be performed on the output of the last (smallest resolution)
            FPN output, and the result will extend the result list. The top_block
            further downsamples the feature map. It must have an attribute
            "num_levels", meaning the number of extra FPN levels added by
            this block, and "in_feature", which is a string representing
            its input feature (e.g., p5).
        fuse_type (str): types for fusing the top down features and the lateral
            ones. It can be "sum" (default), which sums up element-wise; or "avg",
            which takes the element-wise mean of the two.
    """
    super(BiFPN, self).__init__()
    assert isinstance(bottom_up, Backbone)

    # Feature map strides and channels from the bottom up network (e.g. ResNet)
    in_strides = [bottom_up.out_feature_strides[f] for f in in_features]
    in_channels = [bottom_up.out_feature_channels[f] for f in in_features]

    self.top_block = top_block
    self.in_features = in_features
    self.bottom_up = bottom_up

    # Return feature names are "p<stage>", like ["p2", "p3", ..., "p6"]
    self._out_feature_strides = {f"p{int(math.log2(s))}": s for s in in_strides}

    # top block output feature maps.
    if self.top_block is not None:
      last_stage = int(math.log2(in_strides[-1]))
      for s in range(last_stage, last_stage + self.top_block.num_levels):
        in_strides.append(2 ** (s + 1))
        in_channels.append(out_channels)
        self._out_feature_strides[f"p{s + 1}"] = 2 ** (s + 1)

    _assert_strides_are_log2_contiguous(in_strides)

    self.nodes = [{'out_stride': in_strides[-2], 'inputs_offsets': [3, 4]},
                  {'out_stride': in_strides[-3], 'inputs_offsets': [2, 5]},
                  {'out_stride': in_strides[-4], 'inputs_offsets': [1, 6]},
                  {'out_stride': in_strides[-5], 'inputs_offsets': [0, 7]},
                  {'out_stride': in_strides[-4], 'inputs_offsets': [1, 7, 8]},
                  {'out_stride': in_strides[-3], 'inputs_offsets': [2, 6, 9]},
                  {'out_stride': in_strides[-2], 'inputs_offsets': [3, 5, 10]},
                  {'out_stride': in_strides[-1], 'inputs_offsets': [4, 11]}]

    self.lateral_convs = nn.ModuleList()
    self.attentions = nn.ModuleList()
    self.output_convs = nn.ModuleList()
    for r in range(fpn_repeat):
      _in_channels, _in_strides = deepcopy(in_channels), deepcopy(in_strides)
      self.lateral_convs.append(nn.ModuleList())
      self.attentions.append(nn.ModuleList())
      self.output_convs.append(nn.ModuleList())
      num_output_connections = [0 for _ in in_channels]
      for node in self.nodes:
        self.lateral_convs[-1].append(nn.ModuleList())
        for i in node['inputs_offsets']:
          self.lateral_convs[-1][-1].append(ResampleFeature(
            in_channels=_in_channels[i] if r == 0 else out_channels,
            out_channels=out_channels,
            in_stride=_in_strides[i],
            out_stride=node['out_stride'],
            norm=norm))
          # self.lateral_convs[-1][-1].append(
          #     ResampleFeature_v2(in_channels=_in_channels[i] if r == 0 else out_channels,
          #                        out_channels=out_channels,
          #                        norm=norm))
          num_output_connections[i] += 1
        self.attentions[-1].append(Attention(num_inputs=len(node['inputs_offsets'])))
        self.output_convs[-1].append(OutputConv(out_channels, out_channels, norm))
        _in_channels.append(out_channels)
        _in_strides.append(node['out_stride'])
        num_output_connections.append(0)

    self.in_strides = in_strides
    self._out_features = list(self._out_feature_strides.keys())
    self._out_feature_channels = {k: out_channels for k in self._out_features}
    self._size_divisibility = in_strides[-1]
    assert fuse_type in {"avg", "sum"}
    self._fuse_type = fuse_type

  @property
  def size_divisibility(self):
    return self._size_divisibility

  def forward(self, x):
    """
    Args:
        input (dict[str->Tensor]): mapping feature map name (e.g., "res5") to
            feature map tensor for each feature level in high to low resolution order.
    Returns:
        dict[str->Tensor]:
            mapping from feature map name to FPN feature map tensor
            in high to low resolution order. Returned feature names follow the FPN
            paper convention: "p<stage>", where stage has stride = 2 ** stage e.g.,
            ["p2", "p3", ..., "p6"].
    """
    # Reverse feature maps into top-down order (from low to high resolution)
    bottom_up_features = self.bottom_up(x)
    p5, p4, p3 = [bottom_up_features[f] for f in self.in_features[::-1]]
    features = [p3, p4, p5]
    # get p6, p7
    if self.top_block is not None:
      features.extend(self.top_block(bottom_up_features.get(self.top_block.in_feature)))

    in_sizes = {s: features[i].shape[-2:] for i, s in enumerate(self.in_strides)}

    for lateral_convs, attentions, output_convs in zip(self.lateral_convs,
                                                       self.attentions,
                                                       self.output_convs):
      for node, lateral_conv, attention, output_conv in zip(self.nodes,
                                                            lateral_convs,
                                                            attentions,
                                                            output_convs):
        out = [l_conv(features[i]) for i, l_conv in zip(node['inputs_offsets'], lateral_conv)]
        # out = [l_conv(features[i], in_sizes[node['out_stride']])
        #        for i, l_conv in zip(node['inputs_offsets'], lateral_conv)]
        out = attention(out)
        out = output_conv(out)
        features.append(out)

      new_features = {}
      for s in self.in_strides:
        for i, node in enumerate(reversed(self.nodes)):
          if node['out_stride'] == s:
            new_features[s] = features[-1 - i]
            break
      features = [new_features[s] for s in self.in_strides]

    assert len(self._out_features) == len(features)
    return dict(zip(self._out_features, features))

  def output_shape(self):
    return {name: ShapeSpec(channels=self._out_feature_channels[name],
                            stride=self._out_feature_strides[name])
            for name in self._out_features}


def _assert_strides_are_log2_contiguous(strides):
  """
  Assert that each stride is 2x times its preceding stride, i.e. "contiguous in log2".
  """
  for i, stride in enumerate(strides[1:], 1):
    assert stride == 2 * strides[i - 1], \
      f"Strides {stride} {strides[i - 1]} are not log2 contiguous"


class LastLevelP6P7(nn.Module):
  """
  This module is used in RetinaNet to generate extra layers, P6 and P7 from
  C5 feature.
  """

  def __init__(self, in_channels, out_channels, in_feature="res5", norm=''):
    super().__init__()
    self.num_levels = 2
    self.in_feature = in_feature
    self.p6 = ResampleFeature(in_channels, out_channels, in_stride=1, out_stride=2, norm=norm)
    self.p7 = ResampleFeature(out_channels, out_channels, in_stride=1, out_stride=2, norm=norm)

  def forward(self, p5):
    p6 = self.p6(p5)
    p7 = self.p7(p6)
    return [p6, p7]


# @BACKBONE_REGISTRY.register()
# def build_resnet_bifpn_backbone(cfg, input_shape: ShapeSpec):
#     """
#     Args:
#         cfg: a detectron2 CfgNode
#     Returns:
#         backbone (Backbone): backbone module, must be a subclass of :class:`Backbone`.
#     """
#     bottom_up = build_resnet_backbone(cfg, input_shape)
#     in_features = cfg.MODEL.FPN.IN_FEATURES
#     out_channels = cfg.MODEL.FPN.OUT_CHANNELS
#     backbone = BiFPN(
#         bottom_up=bottom_up,
#         in_features=in_features,
#         out_channels=out_channels,
#         norm=cfg.MODEL.FPN.NORM,
#         top_block=LastLevelMaxPool(),
#         fuse_type=cfg.MODEL.FPN.FUSE_TYPE,
#     )
#     return backbone


@BACKBONE_REGISTRY.register()
def build_retinanet_resnet_bifpn_backbone(cfg, input_shape: ShapeSpec):
  """
  Args:
      cfg: a detectron2 CfgNode
  Returns:
      backbone (Backbone): backbone module, must be a subclass of :class:`Backbone`.
  """
  bottom_up = build_resnet_backbone(cfg, input_shape)
  in_features = cfg.MODEL.FPN.IN_FEATURES
  out_channels = cfg.MODEL.FPN.OUT_CHANNELS
  in_feature_p6p7 = cfg.MODEL.FPN.IN_FEATURE_P6P7
  in_channels_p6p7 = bottom_up.output_shape()[in_feature_p6p7].channels
  backbone = BiFPN(bottom_up=bottom_up,
                   in_features=in_features,
                   out_channels=out_channels,
                   fpn_repeat=cfg.MODEL.FPN.REPEAT,
                   norm=cfg.MODEL.FPN.NORM,
                   top_block=LastLevelP6P7(in_channels_p6p7,
                                           out_channels,
                                           in_feature_p6p7,
                                           cfg.MODEL.FPN.NORM),
                   fuse_type=cfg.MODEL.FPN.FUSE_TYPE)
  return backbone


@BACKBONE_REGISTRY.register()
def build_retinanet_efficientnet_bifpn_backbone(cfg, input_shape: ShapeSpec):
  """
  Args:
      cfg: a detectron2 CfgNode
  Returns:
      backbone (Backbone): backbone module, must be a subclass of :class:`Backbone`.
  """
  bottom_up = build_efficientnet_backbone(cfg, input_shape)
  in_features = cfg.MODEL.FPN.IN_FEATURES
  out_channels = cfg.MODEL.FPN.OUT_CHANNELS
  in_feature_p6p7 = cfg.MODEL.FPN.IN_FEATURE_P6P7
  in_channels_p6p7 = bottom_up.output_shape()[in_feature_p6p7].channels
  backbone = BiFPN(bottom_up=bottom_up,
                   in_features=in_features,
                   out_channels=out_channels,
                   fpn_repeat=cfg.MODEL.FPN.REPEAT,
                   norm=cfg.MODEL.FPN.NORM,
                   top_block=LastLevelP6P7(in_channels_p6p7,
                                           out_channels,
                                           in_feature_p6p7,
                                           cfg.MODEL.FPN.NORM),
                   fuse_type=cfg.MODEL.FPN.FUSE_TYPE)
  return backbone
